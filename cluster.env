# vLLM + Ray (official run_cluster.sh) configuration
# Network
HEAD_NODE_IP=127.0.0.1
RAY_ADDRESS=127.0.0.1:6379

# Set this per-machine (see below):
#VLLM_HOST_IP=...
VLLM_PORT=8000

# Docker image
VLLM_IMAGE=vllm/vllm-openai:latest

# Model to serve (small for initial validation; change later)
#MODEL=Qwen/Qwen2.5-1.5B-Instruct
MODEL=Qwen/Qwen2.5-0.5B-Instruct

# HuggingFace cache directory on the HOST (bind-mounted into container)
HF_HOME=${HOME}/.cache/huggingface

PIPELINE_PARALLEL_SIZE=1
TENSOR_PARALLEL_SIZE=1
DATA_PARALLEL_SIZE=1
GPU_MEMORY_UTILIZATION=0.2
MAX_MODEL_LEN=8192
MAX_NUM_SEQS=16
VLLM_TOOL_ARGS="--enable-auto-tool-choice --tool-call-parser hermes"
# Optional: Hugging Face token (needed for gated/private models)
# export HF_TOKEN=hf_xxx...
